# 障害対応手順書 (Incident Response Procedures)

**バージョン:** 1.0
**最終更新日:** 2026年2月15日
**対象システム:** Mirai DX Management System
**運用体制:** IT部門 5名体制

---

## 1. はじめに

本ドキュメントは、Mirai DX Management System で障害が発生した場合の対応手順を定義します。迅速な復旧と最小限のビジネス影響を実現するため、障害レベルに応じたエスカレーション、対応体制、復旧手順を規定します。

---

## 2. 障害レベル定義

### 2.1 障害レベルの分類

| レベル | 定義 | ユーザー影響 | 対応時間 | 対応体制 |
|--------|------|-----------|---------|---------|
| **Level 1 (Critical)** | 全システムダウン、全ユーザー利用不可 | 全組織が業務停止 | **15分以内** | 全IT部門 + ベンダー |
| **Level 2 (High)** | 主要機能が利用不可、一部ユーザー影響 | 特定部門が業務停止 | **1時間以内** | 主務2名 + セキュリティ担当 |
| **Level 3 (Medium)** | 機能の一部が低速/不安定 | 軽微な業務遅延 | **2時間以内** | 主務1名 |
| **Level 4 (Low)** | 軽微なバグ、UI不具合 | ユーザーが工夫で対応可能 | **24時間以内** | 副務1名 |

### 2.2 障害レベルの判定基準

```
判定フロー:
1. システムダウン？
   └─ YES → Level 1
   └─ NO ↓

2. 主要ビジネス機能が利用不可？
   └─ YES → Level 2
   └─ NO ↓

3. 複数のユーザーに影響がある？
   └─ YES → Level 3
   └─ NO ↓

4. 単一ユーザー、軽微な影響
   └─ Level 4
```

**判定例：**
- 全ユーザーがログインできない → **Level 1**
- 売上レポート機能が表示されない → **Level 2**
- 特定のユーザーだけレポート出力が遅い → **Level 3**
- ボタンの表示がズレている → **Level 4**

---

## 3. エスカレーションフロー

### 3.1 障害検知から対応までのフロー

```
┌──────────────────────────────────────┐
│  障害検知                             │
│  (アラート / ユーザー報告 / 監視)      │
└───────────┬──────────────────────────┘
            │
            ▼
┌──────────────────────────────────────┐
│  初期判断・レベル決定 (1-2分)          │
│  - 影響範囲確認                        │
│  - ユーザー数確認                      │
│  - Level 1-4 を決定                  │
└───────────┬──────────────────────────┘
            │
            ▼
    ┌───────┴────────┬─────────┬──────────┐
    │                │         │          │
   Level 1         Level 2   Level 3    Level 4
    │                │         │          │
    ▼                ▼         ▼          ▼
全員召集         主務2名     主務1名      副務1名
15分目標         1時間目標   2時間目標   24時間目標
```

### 3.2 Level 1 (Critical) エスカレーション

**発生直後（0～5分）:**

1. **アラート受信**
   - 監視ツール (New Relic/Datadog) が自動検知
   - Slack に自動通知

2. **初期対応の実施**
   ```bash
   # 直ちに実施
   - Slack #incident チャンネルで障害宣言
   - 運用管理者に電話連絡
   - 主務運用エンジニア2名に電話/SMS
   ```

3. **Incident Channel 作成**
   ```
   Slack での投稿内容:

   🚨 **INCIDENT DECLARED - Level 1**

   時刻: [ISO 8601形式]
   影響: [全システムダウン / 主要機能停止 等]
   ユーザー数: [推定]

   対応者:
   - リード: [名前]
   - 対応者2: [名前]
   - 対応者3: [名前]

   更新予定: 15分ごと
   ```

**5～15分:**

1. **応急対応開始**
   - リード運用エンジニアが現場対応
   - 副務運用エンジニアがログ分析
   - セキュリティ担当がセキュリティ脅威確認

2. **初期原因推定**
   ```
   確認対象:
   [ ] サーバーの起動状況
   [ ] ネットワーク接続状況
   [ ] データベース接続状況
   [ ] アプリケーションプロセス状況
   [ ] ディスク容量 / メモリ使用率
   [ ] 直近のデプロイ / 設定変更
   ```

3. **対外通知**
   - ユーザーへの初報通知（Slack / Email）
   - 経営層への報告

**15分以降:**

1. **復旧手段の判断**
   - 再起動で復旧可能？ → 実行
   - ロールバックが必要？ → 3.4 参照
   - ベンダーサポート必要？ → エスカレーション

2. **ベンダーエスカレーション実施**
   ```
   AWS / その他マネージドサービス:
   - AWS Support に Severity 1 ケースをオープン
   - ベンダーの対応開始まで待機

   アプリケーション開発ベンダー:
   - 開発責任者に直ちに連絡
   - ホットフィックスの準備要請
   ```

### 3.3 Level 2 (High) エスカレーション

**発生直後（0～10分）:**

1. **Slack にて報告**
   ```
   ⚠️ **INCIDENT - Level 2**

   機能: [影響を受けた機能名]
   影響ユーザー: [推定]
   対応者: [名前]
   初期対応開始時刻: [時刻]
   ```

2. **主務2名による対応開始**
   - 1名がダッシュボード確認・原因調査
   - 1名がユーザーサポート対応

3. **1時間以内に状況報告**

### 3.4 Level 3 / 4 エスカレーション

**報告のみ Slack に投稿**
- 優先度は朝礼で確認
- 本日中の対応を目指す

---

## 4. 主要障害シナリオと対処法

### 4.1 シナリオ 1: 全サーバーダウン

**原因の可能性:**
- 電源障害 / ネットワーク障害
- 物理サーバーのハードウェア故障
- 全体的なリソース枯渇

**対応手順:**

```
Step 1: 物理的な接続確認 (1-2分)
├─ UPS バッテリー状況を確認
├─ ネットワークケーブルの接続を確認
├─ ネットワークスイッチの LED 状況を確認
└─ AWS EC2 インスタンスが Running 状態か確認

Step 2: ネットワーク疎通確認 (2-3分)
├─ ping コマンドで疎通確認
├─ traceroute でルート確認
└─ DNS 名前解決確認

Step 3: アプリケーション層の復旧 (3-5分)
├─ 管理者ページでサーバーステータス確認
├─ 必要に応じてサーバーを再起動
└─ アプリケーションプロセスの起動確認

Step 4: 復旧確認 (1分)
└─ ユーザーが実際にアクセス可能か確認
```

**実行コマンド:**

```bash
# ネットワーク疎通確認
ping -c 3 google.com
ping -c 3 <primary-server-ip>

# サーバーの稼働状況
systemctl status mirai-dx-app
systemctl status mysql
systemctl status nginx

# サーバー再起動（必要な場合）
sudo systemctl restart mirai-dx-app
sudo systemctl restart mysql

# ヘルスチェック
curl -s http://localhost:8080/health | jq .
```

**復旧見込み時間:**
- ネットワーク障害: 5～10分
- アプリケーション再起動: 3～5分
- ハードウェア故障: 30分～数時間（ベンダー対応）

---

### 4.2 シナリオ 2: データベース応答不可

**原因の可能性:**
- DB プロセスのクラッシュ
- DB ロック / デッドロック
- ディスク容量満杯
- メモリ枯渇
- スローコエリによる負荷

**対応手順:**

```
Step 1: DB 接続確認 (1分)
└─ mysql -h <db-host> -u root -p -e "SELECT 1;"

Step 2: DB プロセス状況確認 (1-2分)
├─ ps aux | grep mysql
├─ systemctl status mysql
└─ MySQL ログを確認

Step 3: 接続数確認 (1分)
└─ mysql -e "SHOW PROCESSLIST;" | wc -l

Step 4: ディスク容量確認 (1分)
└─ df -h
└─ du -sh /var/lib/mysql

Step 5: ロック状況確認 (2分)
└─ mysql -e "SHOW OPEN TABLES WHERE In_use > 0;"

Step 6: 復旧アクション (3-10分)
├─ ロック状態: スローコエリをキルして復旧
├─ 容量満杯: ログアーカイブ + 再起動
├─ メモリ不足: プロセス再起動
└─ クラッシュ: systemctl restart mysql
```

**実行コマンド:**

```bash
# DB 接続テスト
mysql -h <db-host> -u root -p -e "SELECT VERSION();"

# 接続数確認
mysql -e "SHOW PROCESSLIST \G" | grep "Command:" | grep -v Sleep | head -20

# スローコエリの確認と KILL
mysql -e "SHOW PROCESSLIST \G" | grep -A 10 "Command: Query" | grep "Id:" | awk '{print $NF}' | xargs -I {} mysql -e "KILL {};"

# ディスク容量確認
df -h /var/lib/mysql

# DB ログの確認
tail -n 100 /var/log/mysql/error.log | grep -i error
```

**復旧見込み時間:**
- スローコエリキル: 2～5分
- 接続池リセット: 3分
- DB 再起動: 5～10分
- ディスク拡張: 30分～1時間

---

### 4.3 シナリオ 3: アプリケーション異常終了

**原因の可能性:**
- Out of Memory (OOM)
- 未処理例外 / Null Pointer Exception
- 外部依存サービス接続失敗
- メモリリーク

**対応手順:**

```
Step 1: プロセス状況確認 (1分)
├─ ps aux | grep mirai-dx-app
├─ systemctl status mirai-dx-app
└─ top で CPU / メモリを確認

Step 2: アプリケーションログ確認 (2-3分)
└─ tail -n 100 /var/log/mirai-dx/application.log | tail -n 50

Step 3: メモリ使用率確認 (1分)
├─ free -h
└─ jmap -heap <PID> （Java アプリの場合）

Step 4: 復旧アクション (1-5分)
├─ メモリ不足: キャッシュクリア + 再起動
├─ 例外エラー: ログで原因特定後、修正版デプロイ
└─ 依存サービス: 外部サービス復旧待機
```

**実行コマンド:**

```bash
# アプリケーション再起動
sudo systemctl restart mirai-dx-app
sudo systemctl status mirai-dx-app

# メモリ使用率確認
free -h
ps aux --sort=-%mem | head -15

# ログ確認 (最直近エラー)
tail -n 100 /var/log/mirai-dx/application.log | grep -i error | tail -n 10

# ヘルスチェック
curl -s http://localhost:8080/health | jq .
```

**復旧見込み時間:**
- プロセス再起動: 2～5分
- キャッシュクリア + 再起動: 5～10分
- ホットフィックスデプロイ: 15～30分

---

### 4.4 シナリオ 4: ディスク満杯

**原因の可能性:**
- ログファイルの肥大化
- 一時ファイルの蓄積
- バックアップの蓄積

**対応手順:**

```
Step 1: 容量確認 (1分)
├─ df -h
└─ du -sh /* | sort -rh

Step 2: 最大ディレクトリを特定 (1-2分)
└─ du -sh /var/log/* /tmp/* /backup/* | sort -rh

Step 3: ログの圧縮・削除 (2-5分)
├─ 古いログをアーカイブ
├─ 7日以上前のログを削除
└─ 一時ファイル (/tmp) をクリア

Step 4: 必要に応じてディスク拡張
└─ クラウド環境の場合は追加ストレージ割り当て
```

**実行コマンド:**

```bash
# ディスク容量確認
df -h

# ディレクトリ別容量確認
du -sh /* | sort -rh | head -10

# ログファイルの確認
du -sh /var/log/* | sort -rh

# 古いログの削除 (7日以上前)
find /var/log/mirai-dx -name "*.log.*" -mtime +7 -delete

# ディスク容量確認（削除後）
df -h
```

**復旧見込み時間:**
- ログ削除: 5～15分
- ディスク拡張（クラウド）: 10～20分

---

### 4.5 シナリオ 5: 高トラフィック / DDoS 攻撃

**原因の可能性:**
- 想定外のアクセス急増
- DDoS 攻撃
- クローリング bot の暴走

**対応手順:**

```
Step 1: トラフィック確認 (1分)
├─ ネットワークトラフィック確認
├─ リクエスト数確認
└─ 送信元 IP 確認

Step 2: アクセスログ分析 (2-5分)
├─ 特定の IP からの大量リクエスト
├─ 特定の URI への集中
└─ User-Agent 確認

Step 3: 対応アクション (1-10分)
├─ 通常のアクセス: スケールアウト / リソース追加
├─ 疑わしいIP: ファイアウォールで遮断
└─ DDoS 攻撃: CDN / WAF でフィルタ
```

**実行コマンド:**

```bash
# リクエスト数確認 (アプリケーションのアクセスログ)
tail -n 1000 /var/log/mirai-dx/access.log | awk '{print $1}' | sort | uniq -c | sort -rn | head -20

# 特定 IP からのリクエスト確認
grep "203.0.113.50" /var/log/mirai-dx/access.log | wc -l

# ネットワークトラフィック確認
nethogs -d 1

# ファイアウォール設定確認
sudo iptables -L -n | grep 203.0.113.50

# 疑わしい IP をブロック
sudo iptables -I INPUT -s 203.0.113.50 -j DROP
```

**復旧見込み時間:**
- 一般的なトラフィック増: 10～30分（スケールアウト）
- DDoS 攻撃: CDN / WAF 設定で 5～15分

---

### 4.6 シナリオ 6: デプロイ失敗

**原因の可能性:**
- デプロイスクリプトの不具合
- 依存ライブラリの互換性問題
- データベーススキーマ変更の失敗
- リソース不足

**対応手順:**

```
Step 1: デプロイログ確認 (2-3分)
└─ tail -n 200 /var/log/deployment.log

Step 2: アプリケーション状況確認 (1分)
├─ systemctl status mirai-dx-app
└─ curl http://localhost:8080/health

Step 3: 復旧方法を判定 (1-2分)
├─ 新バージョンで復旧可能か
├─ ロールバック後に復旧か
└─ 修正デプロイ必要か

Step 4: 実行 (5-30分)
├─ デプロイ再実行 / 修正デプロイ
└─ ロールバック（必要な場合）
```

**復旧見込み時間:**
- デプロイ再実行: 5～10分
- ロールバック: 10～15分

---

## 5. ロールバック手順

### 5.1 ロールバック判定

**以下のいずれかに該当する場合、ロールバック実施:**

```
[ ] デプロイ直後から障害が発生している
[ ] 新バージョンのバグが明らかである
[ ] 復旧に1時間以上かかることが確定している
[ ] ユーザーへの影響が大きい
```

### 5.2 ロールバック手順

**前提条件:**
- 前バージョンの docker image / デプロイパッケージが保存されている
- データベースマイグレーション時は逆マイグレーションスクリプトが用意されている

**実行手順:**

```bash
# Step 1: 現在のバージョン確認
curl http://localhost:8080/api/version

# Step 2: 稼働中のコンテナを停止
docker stop mirai-dx-app

# Step 3: 前バージョンイメージを確認
docker images | grep mirai-dx-app

# Step 4: 前バージョンを起動
docker run -d --name mirai-dx-app-rollback \
  -e ENV=production \
  -v /data:/app/data \
  mirai-dx-app:v1.2.3  # 前バージョンタグ

# Step 5: ヘルスチェック
curl -s http://localhost:8080/health | jq .

# Step 6: ログ確認 (エラーがないか)
docker logs mirai-dx-app-rollback | tail -n 50 | grep -i error

# Step 7: 復旧確認
# ユーザーにシステムが正常に復旧したことを案内
```

**データベースロールバック:**

```bash
# マイグレーション履歴確認
ls -la /var/lib/db-migrations/

# 逆マイグレーション実行
mysql -u root -p mirai_dx < /var/lib/db-migrations/rollback_v1.3.0.sql

# 確認
mysql -u root -p -e "use mirai_dx; SHOW TABLES; DESC <table_name>;"
```

**ロールバック後の処理:**

```
1. ユーザーに復旧完了を通知
2. 障害原因の根本調査を開始
3. 対象バージョンの修正版作成
4. QA テストの実施
5. 改めてデプロイ実施
```

---

## 6. 復旧手順

### 6.1 障害の段階的復旧

```
┌─ 即座の対応 (0-15分)
│  ├─ ユーザーへ状況通知
│  ├─ 初期原因特定
│  └─ 応急対応実施
│
├─ 本格的な復旧 (15分-2時間)
│  ├─ 根本原因特定
│  ├─ 修正または交換
│  └─ 動作確認
│
└─ 事後対応 (復旧後)
   ├─ ユーザーへの詳細説明
   ├─ 障害レポート作成
   ├─ 根本原因対策の実施
   └─ ナレッジベース更新
```

### 6.2 復旧プロセス

**復旧フェーズ 1: システムの安定化（0～30分）**

```
1. サービスの可用性確認
   - API 応答時間測定
   - エラーレート確認
   - ユーザー接続数確認

2. 必要なサービスの再起動
   - アプリケーション起動
   - DB 起動
   - キャッシュサーバー起動

3. 簡易的な自動復旧
   - スクリプトの自動実行
   - セルフヒーリング機能の起動
```

**復旧フェーズ 2: 詳細な検証（30分～2時間）**

```
1. データ整合性確認
   - DB レプリケーション遅延の確認
   - 破損データの検出
   - ロック状態の確認

2. ユーザーデータの確認
   - 直近の取引が正常に記録されているか
   - キャッシュデータとDB の一致確認

3. パフォーマンス確認
   - 応答時間が正常範囲か
   - エラーログの減少
   - リソース使用率の正常化
```

**復旧フェーズ 3: 完全復旧（2時間～）**

```
1. 全機能の動作確認
   - トランザクション処理テスト
   - レポート出力テスト
   - 外部連携テスト

2. ユーザー受け入れテスト (UAT)
   - 実際のビジネスシナリオでテスト
   - ユーザーが確認

3. 復旧宣言
   - システム正常化を確認
   - ユーザーへ復旧完了を通知
```

### 6.3 復旧確認のチェックリスト

```
[ ] システムにアクセスできる
[ ] ログインできる
[ ] 主要機能が利用可能
[ ] データベースクエリが応答する
[ ] 応答時間が想定範囲内 (<500ms)
[ ] エラーログに重大エラーがない
[ ] リソース使用率が正常範囲
[ ] バックアップは最新の状態か
[ ] 監視ツールが正常に動作している
[ ] ユーザーからのエラー報告がない
```

---

## 7. ログ調査手順

### 7.1 ログの構成と役割

| ログ種別 | 保存場所 | 保持期間 | 用途 |
|---------|---------|---------|------|
| **アプリケーション** | `/var/log/mirai-dx/application.log` | 30日 | アプリ例外・エラー追跡 |
| **データベース** | `/var/log/mysql/error.log` | 30日 | DB エラー追跡 |
| **アクセス** | `/var/log/mirai-dx/access.log` | 90日 | HTTP リクエスト追跡 |
| **セキュリティ** | `/var/log/mirai-dx/security.log` | 180日 | 認証・権限関連ログ |
| **システム** | `/var/log/syslog` | 90日 | OS レベルのログ |

### 7.2 ログ検索の基本

```bash
# 特定時間帯のエラーを検索
grep "2026-02-15 14:3[0-9]" /var/log/mirai-dx/application.log | grep -i error

# 特定ユーザーのアクセスログ
grep "user_id=12345" /var/log/mirai-dx/access.log

# 特定の例外クラス
grep "NullPointerException\|SQLException" /var/log/mirai-dx/application.log

# エラーの出現回数
grep -i error /var/log/mirai-dx/application.log | wc -l

# エラーの種類別集計
grep -i error /var/log/mirai-dx/application.log | awk -F: '{print $NF}' | sort | uniq -c | sort -rn
```

### 7.3 主要なエラーパターンと対策

#### パターン 1: Connection Timeout

```
ログ例:
java.net.SocketTimeoutException: Connection timeout after 30000ms
```

**原因:**
- ネットワーク遅延
- DB が応答していない
- ファイアウォール設定

**確認方法:**
```bash
# ネットワーク疎通確認
ping -c 3 <db-server>
telnet <db-server> 3306

# DB 接続テスト
mysql -h <db-server> -u root -p -e "SELECT 1;"
```

#### パターン 2: Out of Memory

```
ログ例:
java.lang.OutOfMemoryError: Java heap space
```

**原因:**
- メモリリーク
- 予期しない大量データ処理

**確認方法:**
```bash
# JVM メモリ設定確認
ps aux | grep java | grep Xmx

# ヒープダンプ取得
jmap -dump:live,format=b,file=/tmp/heap.bin <PID>
```

#### パターン 3: Deadlock Detected

```
ログ例:
Deadlock detected. Rolling back transaction.
```

**原因:**
- トランザクション処理の順序エラー
- 複数ユーザーの同時アクセス

**確認方法:**
```bash
# DB のプロセス確認
mysql -e "SHOW PROCESSLIST \G"

# ロック状態確認
mysql -e "SHOW OPEN TABLES WHERE In_use > 0;"
```

### 7.4 ログ分析ツール

**リアルタイムログ監視:**

```bash
# 新しいエラーをリアルタイムで表示
tail -f /var/log/mirai-dx/application.log | grep -i error

# 特定キーワードの監視
tail -f /var/log/mirai-dx/application.log | grep -E "error|exception|warning"
```

**ログファイルの統計分析:**

```bash
# エラー発生時間帯を調査
grep -i error /var/log/mirai-dx/application.log | awk '{print substr($1, 1, 13)}' | sort | uniq -c

# 最も多く発生しているエラーを特定
grep -i error /var/log/mirai-dx/application.log | awk -F'error' '{print $NF}' | sort | uniq -c | sort -rn | head -10
```

---

## 8. 障害レポートテンプレート

### 8.1 障害レポート記載項目

```markdown
# 障害レポート

## 1. 基本情報

- **障害ID:** INC-2026-0215-001
- **発生日時:** 2026-02-15 14:30:00
- **解決日時:** 2026-02-15 14:48:00
- **障害レベル:** Level 2
- **対応者:** [名前]

## 2. 障害概要

**症状:**
- [ユーザーに見えた症状]

**影響範囲:**
- ユーザー数: 約150名
- 機能: [売上レポート機能]

**ビジネスインパクト:**
- [業務停止、損失額の推定等]

## 3. 原因分析

**根本原因:**
- [原因の詳細記述]

**直接原因:**
- [直接的な問題]

**間接要因:**
- [背景にあった問題]

## 4. 対応履歴

| 時刻 | 対応内容 | 実施者 |
|------|--------|--------|
| 14:30 | アラート受信 | 監視システム |
| 14:32 | Level 2 宣言 | 主務運用エンジニア |
| 14:35 | DB 接続テスト → 失敗 | 主務運用エンジニア |
| 14:40 | スロークエリをKILL | 副務運用エンジニア |
| 14:48 | システム復旧完了 | 主務運用エンジニア |

## 5. 復旧手段

- [実施した復旧内容]

## 6. 再発防止対策

- [ ] 対策1 (実施予定日: 202X-0X-0X)
- [ ] 対策2 (実施予定日: 202X-0X-0X)

## 7. 教訓

- [得られた教訓]
```

### 8.2 レポート提出ルール

- **Level 1 障害:** 24時間以内に報告書作成
- **Level 2 障害:** 48時間以内に報告書作成
- **Level 3 以下:** 週次レポートにまとめて記載

---

## 9. エスカレーション連絡先

### 9.1 IT 部門内

| 役職 | 名前 | 電話 | メール | 対応時間 |
|------|------|------|--------|---------|
| 運用管理者 | [名前] | 090-XXXX-XXXX | [email] | 24/7 |
| 主務運用E1 | [名前] | 090-XXXX-XXXX | [email] | 営業時間 |
| 主務運用E2 | [名前] | 090-XXXX-XXXX | [email] | 営業時間 |
| セキュリティ担当 | [名前] | 090-XXXX-XXXX | [email] | 営業時間 |

### 9.2 外部ベンダー

| ベンダー | サービス | 電話 | メール | 対応時間 |
|---------|---------|------|--------|---------|
| AWS | AWS Support | +81-0120-XXXX | support@aws.amazon.com | 24/7 |
| 開発ベンダー | アプリ開発 | 03-XXXX-XXXX | support@dev-vendor.com | 平日9-17 |
| ネットワーク | ISP | 0120-XXXX-XXXX | support@isp.jp | 24/7 |

---

## 10. 関連ドキュメント

- 運用マニュアル (`Operations-Manual.md`)
- 保守計画書 (`Maintenance-Plan.md`)
- システム構築ドキュメント (`../03_設計/`)

---

**記作成:** 2026年2月15日
**最終レビュー:** 2026年2月15日
**次回レビュー予定:** 2027年1月15日
